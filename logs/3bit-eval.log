Starting ...
Ready.
Quantizing layer 1/32..
+------------------+--------------+------------+-----------+-------+
|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |
+==================+==============+============+===========+=======+
| self_attn.k_proj | 884.441      | -          | -         | 1.670 |
| self_attn.v_proj | 34.725       | -          | -         | 1.206 |
| self_attn.q_proj | 1007.768     | -          | -         | 1.181 |
| self_attn.o_proj | 0.697        | -          | -         | 1.485 |
| mlp.up_proj      | 686.934      | -          | -         | 1.546 |
| mlp.gate_proj    | 710.129      | -          | -         | 1.268 |
| mlp.down_proj    | 6.185        | -          | -         | 4.440 |
+------------------+--------------+------------+-----------+-------+


Quantizing layer 2/32..
+------------------+--------------+------------+-----------+-------+
|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |
+==================+==============+============+===========+=======+
| self_attn.k_proj | 3769.502     | -          | -         | 1.588 |
| self_attn.v_proj | 204.275      | -          | -         | 1.259 |
| self_attn.q_proj | 3737.253     | -          | -         | 1.241 |
| self_attn.o_proj | 22.433       | -          | -         | 1.494 |
| mlp.up_proj      | 2813.403     | -          | -         | 1.578 |
| mlp.gate_proj    | 3155.355     | -          | -         | 1.271 |
| mlp.down_proj    | 8028.470     | -          | -         | 4.376 |
+------------------+--------------+------------+-----------+-------+


Quantizing layer 3/32..
+------------------+--------------+------------+-----------+-------+
|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |
+==================+==============+============+===========+=======+
| self_attn.k_proj | 12033.223    | -          | -         | 1.601 |
| self_attn.v_proj | 3048.247     | -          | -         | 1.259 |
| self_attn.q_proj | 10918.675    | -          | -         | 1.213 |
| self_attn.o_proj | 49.456       | -          | -         | 1.482 |
| mlp.up_proj      | 7375.535     | -          | -         | 1.545 |
| mlp.gate_proj    | 8459.791     | -          | -         | 1.237 |
| mlp.down_proj    | 154.905      | -          | -         | 4.399 |
+------------------+--------------+------------+-----------+-------+


Quantizing layer 4/32..
+------------------+--------------+------------+-----------+-------+
|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |
+==================+==============+============+===========+=======+
| self_attn.k_proj | 37763.406    | -          | -         | 1.589 |
| self_attn.v_proj | 9974.322     | -          | -         | 1.235 |
| self_attn.q_proj | 35395.773    | -          | -         | 1.243 |
| self_attn.o_proj | 72.252       | -          | -         | 1.497 |
| mlp.up_proj      | 13950.492    | -          | -         | 1.499 |
| mlp.gate_proj    | 16244.037    | -          | -         | 1.209 |
| mlp.down_proj    | 324.774      | -          | -         | 4.311 |
+------------------+--------------+------------+-----------+-------+


Quantizing layer 5/32..
+------------------+--------------+------------+-----------+-------+
|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |
+==================+==============+============+===========+=======+
| self_attn.k_proj | 36026.703    | -          | -         | 1.565 |
| self_attn.v_proj | 9966.407     | -          | -         | 1.231 |
| self_attn.q_proj | 34753.895    | -          | -         | 1.220 |
| self_attn.o_proj | 137.049      | -          | -         | 1.465 |
| mlp.up_proj      | 18242.125    | -          | -         | 1.549 |
| mlp.gate_proj    | 22412.641    | -          | -         | 1.252 |
| mlp.down_proj    | 587.780      | -          | -         | 4.326 |
+------------------+--------------+------------+-----------+-------+


Quantizing layer 6/32..
+------------------+--------------+------------+-----------+-------+
|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |
+==================+==============+============+===========+=======+
| self_attn.k_proj | 43733.941    | -          | -         | 1.537 |
| self_attn.v_proj | 12184.842    | -          | -         | 1.220 |
| self_attn.q_proj | 40173.273    | -          | -         | 1.196 |
| self_attn.o_proj | 281.127      | -          | -         | 1.502 |
| mlp.up_proj      | 23172.398    | -          | -         | 1.558 |
| mlp.gate_proj    | 28836.291    | -          | -         | 1.276 |
| mlp.down_proj    | 918.014      | -          | -         | 4.367 |
+------------------+--------------+------------+-----------+-------+


Quantizing layer 7/32..
+------------------+--------------+------------+-----------+-------+
|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |
+==================+==============+============+===========+=======+
| self_attn.k_proj | 62217.859    | -          | -         | 1.534 |
| self_attn.v_proj | 17646.750    | -          | -         | 1.282 |
| self_attn.q_proj | 60363.359    | -          | -         | 1.285 |
| self_attn.o_proj | 374.147      | -          | -         | 1.527 |
| mlp.up_proj      | 28718.611    | -          | -         | 1.583 |
| mlp.gate_proj    | 37170.922    | -          | -         | 1.299 |
| mlp.down_proj    | 1355.548     | -          | -         | 4.341 |
+------------------+--------------+------------+-----------+-------+


Quantizing layer 8/32..
+------------------+--------------+------------+-----------+-------+
|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |
+==================+==============+============+===========+=======+
| self_attn.k_proj | 67531.375    | -          | -         | 1.575 |
| self_attn.v_proj | 20284.551    | -          | -         | 1.274 |
| self_attn.q_proj | 66974.188    | -          | -         | 1.231 |
| self_attn.o_proj | 509.033      | -          | -         | 1.497 |
| mlp.up_proj      | 34119.715    | -          | -         | 1.553 |
| mlp.gate_proj    | 43819.832    | -          | -         | 1.270 |
| mlp.down_proj    | 1814.785     | -          | -         | 4.324 |
+------------------+--------------+------------+-----------+-------+


Quantizing layer 9/32..
+------------------+--------------+------------+-----------+-------+
|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |
+==================+==============+============+===========+=======+
| self_attn.k_proj | 68284.500    | -          | -         | 1.587 |
| self_attn.v_proj | 20995.594    | -          | -         | 1.261 |
| self_attn.q_proj | 67143.719    | -          | -         | 1.252 |
| self_attn.o_proj | 796.528      | -          | -         | 1.506 |
| mlp.up_proj      | 37279.180    | -          | -         | 1.576 |
| mlp.gate_proj    | 45292.961    | -          | -         | 1.261 |
| mlp.down_proj    | 2190.964     | -          | -         | 4.251 |
+------------------+--------------+------------+-----------+-------+


Quantizing layer 10/32..
+------------------+--------------+------------+-----------+-------+
|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |
+==================+==============+============+===========+=======+
| self_attn.k_proj | 75657.578    | -          | -         | 1.583 |
| self_attn.v_proj | 23305.381    | -          | -         | 1.253 |
| self_attn.q_proj | 71040.656    | -          | -         | 1.252 |
| self_attn.o_proj | 1124.713     | -          | -         | 1.521 |
| mlp.up_proj      | 41249.836    | -          | -         | 1.585 |
| mlp.gate_proj    | 48476.453    | -          | -         | 1.280 |
| mlp.down_proj    | 2610.756     | -          | -         | 4.343 |
+------------------+--------------+------------+-----------+-------+


Quantizing layer 11/32..
+------------------+--------------+------------+-----------+-------+
|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |
+==================+==============+============+===========+=======+
| self_attn.k_proj | 80694.359    | -          | -         | 1.568 |
| self_attn.v_proj | 24242.992    | -          | -         | 1.180 |
| self_attn.q_proj | 75061.094    | -          | -         | 1.256 |
| self_attn.o_proj | 1554.815     | -          | -         | 1.494 |
| mlp.up_proj      | 44889.121    | -          | -         | 1.557 |
| mlp.gate_proj    | 51394.012    | -          | -         | 1.253 |
| mlp.down_proj    | 3053.928     | -          | -         | 4.378 |
+------------------+--------------+------------+-----------+-------+


Quantizing layer 12/32..
+------------------+--------------+------------+-----------+-------+
|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |
+==================+==============+============+===========+=======+
| self_attn.k_proj | 84881.281    | -          | -         | 1.592 |
| self_attn.v_proj | 32870.152    | -          | -         | 1.218 |
| self_attn.q_proj | 84319.453    | -          | -         | 1.246 |
| self_attn.o_proj | 1695.749     | -          | -         | 1.503 |
| mlp.up_proj      | 49560.836    | -          | -         | 1.562 |
| mlp.gate_proj    | 55383.117    | -          | -         | 1.254 |
| mlp.down_proj    | 3486.260     | -          | -         | 4.380 |
+------------------+--------------+------------+-----------+-------+


Quantizing layer 13/32..
+------------------+--------------+------------+-----------+-------+
|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |
+==================+==============+============+===========+=======+
| self_attn.k_proj | 95015.516    | -          | -         | 1.601 |
| self_attn.v_proj | 32420.471    | -          | -         | 1.216 |
| self_attn.q_proj | 88469.789    | -          | -         | 1.227 |
| self_attn.o_proj | 1855.939     | -          | -         | 1.486 |
| mlp.up_proj      | 54226.305    | -          | -         | 1.548 |
| mlp.gate_proj    | 58769.508    | -          | -         | 1.255 |
| mlp.down_proj    | 4086.325     | -          | -         | 4.370 |
+------------------+--------------+------------+-----------+-------+


Quantizing layer 14/32..
+------------------+--------------+------------+-----------+-------+
|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |
+==================+==============+============+===========+=======+
| self_attn.k_proj | 95140.477    | -          | -         | 1.575 |
| self_attn.v_proj | 36365.492    | -          | -         | 1.238 |
| self_attn.q_proj | 91216.977    | -          | -         | 1.209 |
| self_attn.o_proj | 2085.559     | -          | -         | 1.491 |
| mlp.up_proj      | 58507.680    | -          | -         | 1.550 |
| mlp.gate_proj    | 61765.648    | -          | -         | 1.268 |
| mlp.down_proj    | 5147.573     | -          | -         | 4.378 |
+------------------+--------------+------------+-----------+-------+


Quantizing layer 15/32..
+------------------+--------------+------------+-----------+-------+
|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |
+==================+==============+============+===========+=======+
| self_attn.k_proj | 98868.641    | -          | -         | 1.566 |
| self_attn.v_proj | 36300.984    | -          | -         | 1.253 |
| self_attn.q_proj | 93224.797    | -          | -         | 1.232 |
| self_attn.o_proj | 2696.445     | -          | -         | 1.484 |
| mlp.up_proj      | 64627.941    | -          | -         | 1.557 |
| mlp.gate_proj    | 67752.430    | -          | -         | 1.271 |
| mlp.down_proj    | 6263.090     | -          | -         | 4.403 |
+------------------+--------------+------------+-----------+-------+


Quantizing layer 16/32..
+------------------+--------------+------------+-----------+-------+
|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |
+==================+==============+============+===========+=======+
| self_attn.k_proj | 96662.578    | -          | -         | 1.578 |
| self_attn.v_proj | 38186.609    | -          | -         | 1.268 |
| self_attn.q_proj | 89660.703    | -          | -         | 1.255 |
| self_attn.o_proj | 3017.732     | -          | -         | 1.509 |
| mlp.up_proj      | 70668.406    | -          | -         | 1.523 |
| mlp.gate_proj    | 74051.703    | -          | -         | 1.235 |
| mlp.down_proj    | 7823.588     | -          | -         | 4.387 |
+------------------+--------------+------------+-----------+-------+


Quantizing layer 17/32..
+------------------+--------------+------------+-----------+-------+
|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |
+==================+==============+============+===========+=======+
| self_attn.k_proj | 99370.656    | -          | -         | 1.547 |
| self_attn.v_proj | 43773.508    | -          | -         | 1.224 |
| self_attn.q_proj | 93410.469    | -          | -         | 1.199 |
| self_attn.o_proj | 3974.448     | -          | -         | 1.471 |
| mlp.up_proj      | 78613.625    | -          | -         | 1.522 |
| mlp.gate_proj    | 83544.297    | -          | -         | 1.237 |
| mlp.down_proj    | 10246.337    | -          | -         | 4.329 |
+------------------+--------------+------------+-----------+-------+


Quantizing layer 18/32..
+------------------+--------------+------------+-----------+-------+
|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |
+==================+==============+============+===========+=======+
| self_attn.k_proj | 100924.844   | -          | -         | 1.549 |
| self_attn.v_proj | 45715.125    | -          | -         | 1.206 |
| self_attn.q_proj | 95691.961    | -          | -         | 1.236 |
| self_attn.o_proj | 2973.395     | -          | -         | 1.487 |
| mlp.up_proj      | 89536.430    | -          | -         | 1.537 |
| mlp.gate_proj    | 97661.188    | -          | -         | 1.250 |
| mlp.down_proj    | 11587.136    | -          | -         | 4.303 |
+------------------+--------------+------------+-----------+-------+


Quantizing layer 19/32..
+------------------+--------------+------------+-----------+-------+
|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |
+==================+==============+============+===========+=======+
| self_attn.k_proj | 109048.688   | -          | -         | 1.571 |
| self_attn.v_proj | 56737.945    | -          | -         | 1.215 |
| self_attn.q_proj | 103948.172   | -          | -         | 1.230 |
| self_attn.o_proj | 3235.440     | -          | -         | 1.471 |
| mlp.up_proj      | 99147.750    | -          | -         | 1.545 |
| mlp.gate_proj    | 110797.719   | -          | -         | 1.244 |
| mlp.down_proj    | 13688.146    | -          | -         | 4.351 |
+------------------+--------------+------------+-----------+-------+


Quantizing layer 20/32..
+------------------+--------------+------------+-----------+-------+
|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |
+==================+==============+============+===========+=======+
| self_attn.k_proj | 104743.797   | -          | -         | 1.577 |
| self_attn.v_proj | 56654.234    | -          | -         | 1.221 |
| self_attn.q_proj | 100279.750   | -          | -         | 1.215 |
| self_attn.o_proj | 3158.341     | -          | -         | 1.455 |
| mlp.up_proj      | 106657.797   | -          | -         | 1.560 |
| mlp.gate_proj    | 120148.617   | -          | -         | 1.250 |
| mlp.down_proj    | 15469.111    | -          | -         | 4.297 |
+------------------+--------------+------------+-----------+-------+


Quantizing layer 21/32..
+------------------+--------------+------------+-----------+-------+
|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |
+==================+==============+============+===========+=======+
| self_attn.k_proj | 107504.586   | -          | -         | 1.508 |
| self_attn.v_proj | 59301.781    | -          | -         | 1.187 |
| self_attn.q_proj | 105241.391   | -          | -         | 1.165 |
| self_attn.o_proj | 4082.795     | -          | -         | 1.486 |
| mlp.up_proj      | 111168.281   | -          | -         | 1.488 |
| mlp.gate_proj    | 126558.195   | -          | -         | 1.251 |
| mlp.down_proj    | 18179.684    | -          | -         | 4.282 |
+------------------+--------------+------------+-----------+-------+


Quantizing layer 22/32..
+------------------+--------------+------------+-----------+-------+
|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |
+==================+==============+============+===========+=======+
| self_attn.k_proj | 110761.305   | -          | -         | 1.534 |
| self_attn.v_proj | 69031.172    | -          | -         | 1.222 |
| self_attn.q_proj | 108357.312   | -          | -         | 1.218 |
| self_attn.o_proj | 3504.239     | -          | -         | 1.514 |
| mlp.up_proj      | 118180.242   | -          | -         | 1.522 |
| mlp.gate_proj    | 136870.344   | -          | -         | 1.212 |
| mlp.down_proj    | 18718.789    | -          | -         | 4.308 |
+------------------+--------------+------------+-----------+-------+


Quantizing layer 23/32..
+------------------+--------------+------------+-----------+-------+
|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |
+==================+==============+============+===========+=======+
| self_attn.k_proj | 119116.133   | -          | -         | 1.563 |
| self_attn.v_proj | 71374.086    | -          | -         | 1.212 |
| self_attn.q_proj | 116230.391   | -          | -         | 1.202 |
| self_attn.o_proj | 4428.068     | -          | -         | 1.496 |
| mlp.up_proj      | 124131.375   | -          | -         | 1.526 |
| mlp.gate_proj    | 145758.531   | -          | -         | 1.241 |
| mlp.down_proj    | 21261.305    | -          | -         | 4.353 |
+------------------+--------------+------------+-----------+-------+


Quantizing layer 24/32..
+------------------+--------------+------------+-----------+-------+
|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |
+==================+==============+============+===========+=======+
| self_attn.k_proj | 129349.828   | -          | -         | 1.572 |
| self_attn.v_proj | 87716.219    | -          | -         | 1.200 |
| self_attn.q_proj | 127792.805   | -          | -         | 1.213 |
| self_attn.o_proj | 4666.618     | -          | -         | 1.465 |
| mlp.up_proj      | 134502.281   | -          | -         | 1.564 |
| mlp.gate_proj    | 156097.031   | -          | -         | 1.244 |
| mlp.down_proj    | 22989.098    | -          | -         | 4.323 |
+------------------+--------------+------------+-----------+-------+


Quantizing layer 25/32..
+------------------+--------------+------------+-----------+-------+
|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |
+==================+==============+============+===========+=======+
| self_attn.k_proj | 120460.547   | -          | -         | 1.557 |
| self_attn.v_proj | 84424.969    | -          | -         | 1.199 |
| self_attn.q_proj | 120508.352   | -          | -         | 1.212 |
| self_attn.o_proj | 4939.370     | -          | -         | 1.518 |
| mlp.up_proj      | 143303.500   | -          | -         | 1.568 |
| mlp.gate_proj    | 166294.703   | -          | -         | 1.275 |
| mlp.down_proj    | 24755.426    | -          | -         | 4.321 |
+------------------+--------------+------------+-----------+-------+


Quantizing layer 26/32..
+------------------+--------------+------------+-----------+-------+
|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |
+==================+==============+============+===========+=======+
| self_attn.k_proj | 140457.406   | -          | -         | 1.522 |
| self_attn.v_proj | 105941.359   | -          | -         | 1.194 |
| self_attn.q_proj | 141346.344   | -          | -         | 1.185 |
| self_attn.o_proj | 3802.538     | -          | -         | 1.468 |
| mlp.up_proj      | 154930.031   | -          | -         | 1.518 |
| mlp.gate_proj    | 179157.688   | -          | -         | 1.222 |
| mlp.down_proj    | 27107.793    | -          | -         | 4.282 |
+------------------+--------------+------------+-----------+-------+


Quantizing layer 27/32..
+------------------+--------------+------------+-----------+-------+
|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |
+==================+==============+============+===========+=======+
| self_attn.k_proj | 133807.672   | -          | -         | 1.556 |
| self_attn.v_proj | 105505.164   | -          | -         | 1.231 |
| self_attn.q_proj | 134123.781   | -          | -         | 1.199 |
| self_attn.o_proj | 6461.681     | -          | -         | 1.460 |
| mlp.up_proj      | 165581       | -          | -         | 1.504 |
| mlp.gate_proj    | 191152.891   | -          | -         | 1.212 |
| mlp.down_proj    | 30336.355    | -          | -         | 4.244 |
+------------------+--------------+------------+-----------+-------+


Quantizing layer 28/32..
+------------------+--------------+------------+-----------+-------+
|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |
+==================+==============+============+===========+=======+
| self_attn.k_proj | 147419.109   | -          | -         | 1.549 |
| self_attn.v_proj | 109959.188   | -          | -         | 1.214 |
| self_attn.q_proj | 146700.156   | -          | -         | 1.203 |
| self_attn.o_proj | 6051.103     | -          | -         | 1.442 |
| mlp.up_proj      | 177843.188   | -          | -         | 1.537 |
| mlp.gate_proj    | 203497.656   | -          | -         | 1.210 |
| mlp.down_proj    | 34453.957    | -          | -         | 4.252 |
+------------------+--------------+------------+-----------+-------+


Quantizing layer 29/32..
+------------------+--------------+------------+-----------+-------+
|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |
+==================+==============+============+===========+=======+
| self_attn.k_proj | 145250.438   | -          | -         | 1.523 |
| self_attn.v_proj | 121375.609   | -          | -         | 1.183 |
| self_attn.q_proj | 144592.969   | -          | -         | 1.180 |
| self_attn.o_proj | 8150.022     | -          | -         | 1.445 |
| mlp.up_proj      | 189669.719   | -          | -         | 1.548 |
| mlp.gate_proj    | 210887.594   | -          | -         | 1.257 |
| mlp.down_proj    | 41208.852    | -          | -         | 4.331 |
+------------------+--------------+------------+-----------+-------+


Quantizing layer 30/32..
+------------------+--------------+------------+-----------+-------+
|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |
+==================+==============+============+===========+=======+
| self_attn.k_proj | 128086.750   | -          | -         | 1.505 |
| self_attn.v_proj | 113654.562   | -          | -         | 1.167 |
| self_attn.q_proj | 129725.625   | -          | -         | 1.177 |
| self_attn.o_proj | 7379.674     | -          | -         | 1.478 |
| mlp.up_proj      | 198067.812   | -          | -         | 1.529 |
| mlp.gate_proj    | 217974       | -          | -         | 1.211 |
| mlp.down_proj    | 49619.211    | -          | -         | 4.190 |
+------------------+--------------+------------+-----------+-------+


Quantizing layer 31/32..
+------------------+--------------+------------+-----------+-------+
|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |
+==================+==============+============+===========+=======+
| self_attn.k_proj | 137114.047   | -          | -         | 1.508 |
| self_attn.v_proj | 125747.234   | -          | -         | 1.171 |
| self_attn.q_proj | 139867.609   | -          | -         | 1.170 |
| self_attn.o_proj | 8859.377     | -          | -         | 1.459 |
| mlp.up_proj      | 199666.594   | -          | -         | 1.519 |
| mlp.gate_proj    | 222585.281   | -          | -         | 1.228 |
| mlp.down_proj    | 83950.508    | -          | -         | 4.240 |
+------------------+--------------+------------+-----------+-------+


Quantizing layer 32/32..
+------------------+--------------+------------+-----------+-------+
|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |
+==================+==============+============+===========+=======+
| self_attn.k_proj | 97882.523    | -          | -         | 1.532 |
| self_attn.v_proj | 68844.641    | -          | -         | 1.167 |
| self_attn.q_proj | 93000.547    | -          | -         | 1.190 |
| self_attn.o_proj | 13206.722    | -          | -         | 1.458 |
| mlp.up_proj      | 167039.703   | -          | -         | 1.509 |
| mlp.gate_proj    | 187384.375   | -          | -         | 1.222 |
| mlp.down_proj    | 158270.094   | -          | -         | 4.233 |
+------------------+--------------+------------+-----------+-------+


752.0970039367676
save quantized tensors.
wikitext2
PPL:  6.417948246002197
ptb
PPL:  876.300537109375
c4
PPL:  7.910240173339844
