Starting ...
Ready.
Quantizing layer 1/32..
+------------------+--------------+------------+-----------+-------+
|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |
+==================+==============+============+===========+=======+
| self_attn.k_proj | 884.441      | -          | -         | 1.684 |
| self_attn.v_proj | 34.725       | -          | -         | 1.216 |
| self_attn.q_proj | 1007.768     | -          | -         | 1.247 |
| self_attn.o_proj | 0.697        | -          | -         | 1.469 |
| mlp.up_proj      | 686.934      | -          | -         | 1.552 |
| mlp.gate_proj    | 710.129      | -          | -         | 1.271 |
| mlp.down_proj    | 6.185        | -          | -         | 4.309 |
+------------------+--------------+------------+-----------+-------+


Quantizing layer 2/32..
+------------------+--------------+------------+-----------+-------+
|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |
+==================+==============+============+===========+=======+
| self_attn.k_proj | 3769.502     | -          | -         | 1.542 |
| self_attn.v_proj | 204.275      | -          | -         | 1.196 |
| self_attn.q_proj | 3737.253     | -          | -         | 1.201 |
| self_attn.o_proj | 22.433       | -          | -         | 1.481 |
| mlp.up_proj      | 2813.403     | -          | -         | 1.558 |
| mlp.gate_proj    | 3155.355     | -          | -         | 1.274 |
quantize to 8 bits
| mlp.down_proj    | 1.720        | -          | -         | 4.370 |
+------------------+--------------+------------+-----------+-------+


Quantizing layer 3/32..
+------------------+--------------+------------+-----------+-------+
|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |
+==================+==============+============+===========+=======+
| self_attn.k_proj | 12066.944    | -          | -         | 1.601 |
| self_attn.v_proj | 3067.919     | -          | -         | 1.255 |
| self_attn.q_proj | 10984.283    | -          | -         | 1.251 |
| self_attn.o_proj | 50.140       | -          | -         | 1.474 |
| mlp.up_proj      | 7595.417     | -          | -         | 1.516 |
| mlp.gate_proj    | 8701.900     | -          | -         | 1.214 |
| mlp.down_proj    | 165.373      | -          | -         | 4.280 |
+------------------+--------------+------------+-----------+-------+


Quantizing layer 4/32..
+------------------+--------------+------------+-----------+-------+
|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |
+==================+==============+============+===========+=======+
| self_attn.k_proj | 38321.945    | -          | -         | 1.587 |
| self_attn.v_proj | 10146.009    | -          | -         | 1.218 |
| self_attn.q_proj | 35964.094    | -          | -         | 1.213 |
| self_attn.o_proj | 95.623       | -          | -         | 1.505 |
| mlp.up_proj      | 14181.833    | -          | -         | 1.545 |
| mlp.gate_proj    | 16502.609    | -          | -         | 1.194 |
| mlp.down_proj    | 348.149      | -          | -         | 4.356 |
+------------------+--------------+------------+-----------+-------+


Quantizing layer 5/32..
+------------------+--------------+------------+-----------+-------+
|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |
+==================+==============+============+===========+=======+
| self_attn.k_proj | 37797.422    | -          | -         | 1.581 |
| self_attn.v_proj | 10437.475    | -          | -         | 1.237 |
| self_attn.q_proj | 36418.785    | -          | -         | 1.265 |
| self_attn.o_proj | 163.076      | -          | -         | 1.475 |
| mlp.up_proj      | 19445.742    | -          | -         | 1.533 |
| mlp.gate_proj    | 23894.434    | -          | -         | 1.196 |
| mlp.down_proj    | 641.324      | -          | -         | 4.292 |
+------------------+--------------+------------+-----------+-------+


Quantizing layer 6/32..
+------------------+--------------+------------+-----------+-------+
|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |
+==================+==============+============+===========+=======+
| self_attn.k_proj | 46001.812    | -          | -         | 1.565 |
| self_attn.v_proj | 12776.395    | -          | -         | 1.215 |
| self_attn.q_proj | 42238.156    | -          | -         | 1.194 |
| self_attn.o_proj | 295.295      | -          | -         | 1.447 |
| mlp.up_proj      | 24212.332    | -          | -         | 1.542 |
| mlp.gate_proj    | 30109.396    | -          | -         | 1.240 |
| mlp.down_proj    | 948.434      | -          | -         | 4.368 |
+------------------+--------------+------------+-----------+-------+


Quantizing layer 7/32..
+------------------+--------------+------------+-----------+-------+
|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |
+==================+==============+============+===========+=======+
| self_attn.k_proj | 63819.703    | -          | -         | 1.547 |
| self_attn.v_proj | 18073.172    | -          | -         | 1.228 |
| self_attn.q_proj | 61943.406    | -          | -         | 1.216 |
| self_attn.o_proj | 360.079      | -          | -         | 1.464 |
| mlp.up_proj      | 29448.463    | -          | -         | 1.540 |
| mlp.gate_proj    | 38117.656    | -          | -         | 1.250 |
| mlp.down_proj    | 1360.101     | -          | -         | 4.346 |
+------------------+--------------+------------+-----------+-------+


Quantizing layer 8/32..
+------------------+--------------+------------+-----------+-------+
|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |
+==================+==============+============+===========+=======+
| self_attn.k_proj | 68403.531    | -          | -         | 1.582 |
| self_attn.v_proj | 20490.738    | -          | -         | 1.239 |
| self_attn.q_proj | 67735.562    | -          | -         | 1.218 |
| self_attn.o_proj | 467.407      | -          | -         | 1.453 |
| mlp.up_proj      | 34457.477    | -          | -         | 1.539 |
| mlp.gate_proj    | 44248.840    | -          | -         | 1.221 |
| mlp.down_proj    | 1778.573     | -          | -         | 4.323 |
+------------------+--------------+------------+-----------+-------+


Quantizing layer 9/32..
+------------------+--------------+------------+-----------+-------+
|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |
+==================+==============+============+===========+=======+
| self_attn.k_proj | 68143.961    | -          | -         | 1.546 |
| self_attn.v_proj | 20936.070    | -          | -         | 1.240 |
| self_attn.q_proj | 67118.438    | -          | -         | 1.263 |
| self_attn.o_proj | 753.051      | -          | -         | 1.504 |
| mlp.up_proj      | 37628.023    | -          | -         | 1.584 |
| mlp.gate_proj    | 45739.680    | -          | -         | 1.240 |
| mlp.down_proj    | 2230.005     | -          | -         | 4.246 |
+------------------+--------------+------------+-----------+-------+


Quantizing layer 10/32..
+------------------+--------------+------------+-----------+-------+
|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |
+==================+==============+============+===========+=======+
| self_attn.k_proj | 76137.047    | -          | -         | 1.570 |
| self_attn.v_proj | 23453.309    | -          | -         | 1.224 |
| self_attn.q_proj | 71552.953    | -          | -         | 1.237 |
| self_attn.o_proj | 1120.847     | -          | -         | 1.471 |
| mlp.up_proj      | 41593.523    | -          | -         | 1.534 |
| mlp.gate_proj    | 48867.355    | -          | -         | 1.229 |
| mlp.down_proj    | 2626.398     | -          | -         | 4.297 |
+------------------+--------------+------------+-----------+-------+


Quantizing layer 11/32..
+------------------+--------------+------------+-----------+-------+
|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |
+==================+==============+============+===========+=======+
| self_attn.k_proj | 81414.078    | -          | -         | 1.528 |
| self_attn.v_proj | 24454.439    | -          | -         | 1.224 |
| self_attn.q_proj | 75684.242    | -          | -         | 1.234 |
| self_attn.o_proj | 1493.991     | -          | -         | 1.471 |
| mlp.up_proj      | 45257.332    | -          | -         | 1.563 |
| mlp.gate_proj    | 51798.844    | -          | -         | 1.252 |
| mlp.down_proj    | 3051.690     | -          | -         | 4.301 |
+------------------+--------------+------------+-----------+-------+


Quantizing layer 12/32..
+------------------+--------------+------------+-----------+-------+
|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |
+==================+==============+============+===========+=======+
| self_attn.k_proj | 85105.781    | -          | -         | 1.565 |
| self_attn.v_proj | 32935.031    | -          | -         | 1.169 |
| self_attn.q_proj | 85146.703    | -          | -         | 1.188 |
| self_attn.o_proj | 1649.668     | -          | -         | 1.477 |
| mlp.up_proj      | 49594.953    | -          | -         | 1.511 |
| mlp.gate_proj    | 55410.777    | -          | -         | 1.229 |
| mlp.down_proj    | 3502.153     | -          | -         | 4.310 |
+------------------+--------------+------------+-----------+-------+


Quantizing layer 13/32..
+------------------+--------------+------------+-----------+-------+
|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |
+==================+==============+============+===========+=======+
| self_attn.k_proj | 95276.875    | -          | -         | 1.566 |
| self_attn.v_proj | 32485.859    | -          | -         | 1.196 |
| self_attn.q_proj | 89257.500    | -          | -         | 1.214 |
| self_attn.o_proj | 1882.106     | -          | -         | 1.493 |
| mlp.up_proj      | 54082.133    | -          | -         | 1.523 |
| mlp.gate_proj    | 58588        | -          | -         | 1.237 |
| mlp.down_proj    | 4108.523     | -          | -         | 4.324 |
+------------------+--------------+------------+-----------+-------+


Quantizing layer 14/32..
+------------------+--------------+------------+-----------+-------+
|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |
+==================+==============+============+===========+=======+
| self_attn.k_proj | 95366.219    | -          | -         | 1.558 |
| self_attn.v_proj | 36393.805    | -          | -         | 1.221 |
| self_attn.q_proj | 91359.047    | -          | -         | 1.213 |
| self_attn.o_proj | 2044.378     | -          | -         | 1.524 |
| mlp.up_proj      | 58554.414    | -          | -         | 1.528 |
| mlp.gate_proj    | 61809.047    | -          | -         | 1.225 |
| mlp.down_proj    | 5096.917     | -          | -         | 4.284 |
+------------------+--------------+------------+-----------+-------+


Quantizing layer 15/32..
+------------------+--------------+------------+-----------+-------+
|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |
+==================+==============+============+===========+=======+
| self_attn.k_proj | 99277.305    | -          | -         | 1.543 |
| self_attn.v_proj | 36416.742    | -          | -         | 1.184 |
| self_attn.q_proj | 94376.156    | -          | -         | 1.203 |
| self_attn.o_proj | 2637.652     | -          | -         | 1.449 |
| mlp.up_proj      | 64836.062    | -          | -         | 1.487 |
| mlp.gate_proj    | 68006.688    | -          | -         | 1.193 |
| mlp.down_proj    | 6208.394     | -          | -         | 4.335 |
+------------------+--------------+------------+-----------+-------+


Quantizing layer 16/32..
+------------------+--------------+------------+-----------+-------+
|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |
+==================+==============+============+===========+=======+
| self_attn.k_proj | 96728.328    | -          | -         | 1.570 |
| self_attn.v_proj | 38182.141    | -          | -         | 1.229 |
| self_attn.q_proj | 90768.539    | -          | -         | 1.221 |
| self_attn.o_proj | 3008.981     | -          | -         | 1.490 |
| mlp.up_proj      | 70620.836    | -          | -         | 1.526 |
| mlp.gate_proj    | 73987.914    | -          | -         | 1.232 |
| mlp.down_proj    | 7745.961     | -          | -         | 4.350 |
+------------------+--------------+------------+-----------+-------+


Quantizing layer 17/32..
+------------------+--------------+------------+-----------+-------+
|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |
+==================+==============+============+===========+=======+
| self_attn.k_proj | 99158.664    | -          | -         | 1.544 |
| self_attn.v_proj | 43674.703    | -          | -         | 1.245 |
| self_attn.q_proj | 93308.500    | -          | -         | 1.193 |
| self_attn.o_proj | 3843.084     | -          | -         | 1.457 |
| mlp.up_proj      | 78605.969    | -          | -         | 1.498 |
| mlp.gate_proj    | 83525.766    | -          | -         | 1.207 |
| mlp.down_proj    | 10199.680    | -          | -         | 4.269 |
+------------------+--------------+------------+-----------+-------+


Quantizing layer 18/32..
+------------------+--------------+------------+-----------+-------+
|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |
+==================+==============+============+===========+=======+
| self_attn.k_proj | 100977.820   | -          | -         | 1.510 |
| self_attn.v_proj | 45737.922    | -          | -         | 1.237 |
| self_attn.q_proj | 95845.359    | -          | -         | 1.225 |
| self_attn.o_proj | 2898.228     | -          | -         | 1.471 |
| mlp.up_proj      | 89454.688    | -          | -         | 1.551 |
| mlp.gate_proj    | 97603.188    | -          | -         | 1.220 |
| mlp.down_proj    | 11665.188    | -          | -         | 4.312 |
+------------------+--------------+------------+-----------+-------+


Quantizing layer 19/32..
+------------------+--------------+------------+-----------+-------+
|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |
+==================+==============+============+===========+=======+
| self_attn.k_proj | 109070.781   | -          | -         | 1.549 |
| self_attn.v_proj | 56800.055    | -          | -         | 1.210 |
| self_attn.q_proj | 104157.078   | -          | -         | 1.261 |
| self_attn.o_proj | 3250.749     | -          | -         | 1.461 |
| mlp.up_proj      | 99095.508    | -          | -         | 1.601 |
| mlp.gate_proj    | 110795.133   | -          | -         | 1.244 |
| mlp.down_proj    | 13818.968    | -          | -         | 4.320 |
+------------------+--------------+------------+-----------+-------+


Quantizing layer 20/32..
+------------------+--------------+------------+-----------+-------+
|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |
+==================+==============+============+===========+=======+
| self_attn.k_proj | 104834.445   | -          | -         | 1.551 |
| self_attn.v_proj | 56663.203    | -          | -         | 1.235 |
| self_attn.q_proj | 100366.672   | -          | -         | 1.248 |
| self_attn.o_proj | 3157.005     | -          | -         | 1.497 |
| mlp.up_proj      | 106835.734   | -          | -         | 1.538 |
| mlp.gate_proj    | 120312.664   | -          | -         | 1.214 |
| mlp.down_proj    | 15616.633    | -          | -         | 4.330 |
+------------------+--------------+------------+-----------+-------+


Quantizing layer 21/32..
+------------------+--------------+------------+-----------+-------+
|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |
+==================+==============+============+===========+=======+
| self_attn.k_proj | 107851.938   | -          | -         | 1.540 |
| self_attn.v_proj | 59519.637    | -          | -         | 1.202 |
| self_attn.q_proj | 105493.453   | -          | -         | 1.196 |
| self_attn.o_proj | 4026.618     | -          | -         | 1.443 |
| mlp.up_proj      | 111354.625   | -          | -         | 1.529 |
| mlp.gate_proj    | 126834.828   | -          | -         | 1.225 |
| mlp.down_proj    | 18298.244    | -          | -         | 4.237 |
+------------------+--------------+------------+-----------+-------+


Quantizing layer 22/32..
+------------------+--------------+------------+-----------+-------+
|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |
+==================+==============+============+===========+=======+
| self_attn.k_proj | 111183.398   | -          | -         | 1.534 |
| self_attn.v_proj | 69277.094    | -          | -         | 1.185 |
| self_attn.q_proj | 108815.875   | -          | -         | 1.421 |
| self_attn.o_proj | 3395.819     | -          | -         | 1.466 |
| mlp.up_proj      | 118921.859   | -          | -         | 1.517 |
| mlp.gate_proj    | 137777.312   | -          | -         | 1.215 |
| mlp.down_proj    | 18982.820    | -          | -         | 4.266 |
+------------------+--------------+------------+-----------+-------+


Quantizing layer 23/32..
+------------------+--------------+------------+-----------+-------+
|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |
+==================+==============+============+===========+=======+
| self_attn.k_proj | 119998.703   | -          | -         | 1.546 |
| self_attn.v_proj | 71893.172    | -          | -         | 1.215 |
| self_attn.q_proj | 119033.102   | -          | -         | 1.228 |
| self_attn.o_proj | 4510.970     | -          | -         | 1.478 |
| mlp.up_proj      | 124885.867   | -          | -         | 1.558 |
| mlp.gate_proj    | 146619.062   | -          | -         | 1.269 |
| mlp.down_proj    | 21504.943    | -          | -         | 4.276 |
+------------------+--------------+------------+-----------+-------+


Quantizing layer 24/32..
+------------------+--------------+------------+-----------+-------+
|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |
+==================+==============+============+===========+=======+
| self_attn.k_proj | 130032.938   | -          | -         | 1.557 |
| self_attn.v_proj | 88232.578    | -          | -         | 1.200 |
| self_attn.q_proj | 128666.547   | -          | -         | 1.215 |
| self_attn.o_proj | 4550.320     | -          | -         | 1.524 |
| mlp.up_proj      | 135466.234   | -          | -         | 1.544 |
| mlp.gate_proj    | 157442.391   | -          | -         | 1.229 |
| mlp.down_proj    | 23304.967    | -          | -         | 4.319 |
+------------------+--------------+------------+-----------+-------+


Quantizing layer 25/32..
+------------------+--------------+------------+-----------+-------+
|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |
+==================+==============+============+===========+=======+
| self_attn.k_proj | 121455.727   | -          | -         | 1.580 |
| self_attn.v_proj | 85113.656    | -          | -         | 1.200 |
| self_attn.q_proj | 121515.219   | -          | -         | 1.219 |
| self_attn.o_proj | 4795.934     | -          | -         | 1.500 |
| mlp.up_proj      | 144577.875   | -          | -         | 1.557 |
| mlp.gate_proj    | 167824.281   | -          | -         | 1.225 |
| mlp.down_proj    | 25091.652    | -          | -         | 4.314 |
+------------------+--------------+------------+-----------+-------+


Quantizing layer 26/32..
+------------------+--------------+------------+-----------+-------+
|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |
+==================+==============+============+===========+=======+
| self_attn.k_proj | 141603.719   | -          | -         | 1.591 |
| self_attn.v_proj | 106876.734   | -          | -         | 1.207 |
| self_attn.q_proj | 142621.812   | -          | -         | 1.197 |
| self_attn.o_proj | 3817.504     | -          | -         | 1.483 |
| mlp.up_proj      | 156250.438   | -          | -         | 1.562 |
| mlp.gate_proj    | 180762.672   | -          | -         | 1.244 |
| mlp.down_proj    | 27382.145    | -          | -         | 4.255 |
+------------------+--------------+------------+-----------+-------+


Quantizing layer 27/32..
+------------------+--------------+------------+-----------+-------+
|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |
+==================+==============+============+===========+=======+
| self_attn.k_proj | 134791.609   | -          | -         | 1.593 |
| self_attn.v_proj | 106314.930   | -          | -         | 1.261 |
| self_attn.q_proj | 135175       | -          | -         | 1.223 |
| self_attn.o_proj | 6345.861     | -          | -         | 1.463 |
| mlp.up_proj      | 167261.125   | -          | -         | 1.535 |
| mlp.gate_proj    | 193199.062   | -          | -         | 1.225 |
| mlp.down_proj    | 30547.230    | -          | -         | 4.309 |
+------------------+--------------+------------+-----------+-------+


Quantizing layer 28/32..
+------------------+--------------+------------+-----------+-------+
|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |
+==================+==============+============+===========+=======+
| self_attn.k_proj | 148945.625   | -          | -         | 1.568 |
| self_attn.v_proj | 110976.133   | -          | -         | 1.227 |
| self_attn.q_proj | 148116.719   | -          | -         | 1.213 |
| self_attn.o_proj | 6082.668     | -          | -         | 1.490 |
| mlp.up_proj      | 179655.328   | -          | -         | 1.547 |
| mlp.gate_proj    | 205820.641   | -          | -         | 1.216 |
| mlp.down_proj    | 34767.844    | -          | -         | 4.288 |
+------------------+--------------+------------+-----------+-------+


Quantizing layer 29/32..
+------------------+--------------+------------+-----------+-------+
|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |
+==================+==============+============+===========+=======+
| self_attn.k_proj | 146636.906   | -          | -         | 1.546 |
| self_attn.v_proj | 122597.273   | -          | -         | 1.240 |
| self_attn.q_proj | 145981.281   | -          | -         | 1.226 |
| self_attn.o_proj | 8176.573     | -          | -         | 1.479 |
| mlp.up_proj      | 191364.438   | -          | -         | 1.566 |
| mlp.gate_proj    | 213014.438   | -          | -         | 1.242 |
| mlp.down_proj    | 41302.594    | -          | -         | 4.327 |
+------------------+--------------+------------+-----------+-------+


Quantizing layer 30/32..
+------------------+--------------+------------+-----------+-------+
|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |
+==================+==============+============+===========+=======+
| self_attn.k_proj | 129179.023   | -          | -         | 1.582 |
| self_attn.v_proj | 114553.812   | -          | -         | 1.241 |
| self_attn.q_proj | 130862.609   | -          | -         | 1.240 |
| self_attn.o_proj | 7611.736     | -          | -         | 1.477 |
quantize to 8 bits
| mlp.up_proj      | 199718.234   | -          | -         | 1.570 |
| mlp.gate_proj    | 163.758      | -          | -         | 1.257 |
| mlp.down_proj    | 48077.699    | -          | -         | 4.352 |
+------------------+--------------+------------+-----------+-------+


Quantizing layer 31/32..
+------------------+--------------+------------+-----------+-------+
|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |
+==================+==============+============+===========+=======+
| self_attn.k_proj | 138020.875   | -          | -         | 1.570 |
| self_attn.v_proj | 126739.219   | -          | -         | 1.240 |
| self_attn.q_proj | 140925.625   | -          | -         | 1.669 |
| self_attn.o_proj | 8922.477     | -          | -         | 1.479 |
| mlp.up_proj      | 200569.281   | -          | -         | 1.555 |
| mlp.gate_proj    | 223569.156   | -          | -         | 1.257 |
| mlp.down_proj    | 85055.492    | -          | -         | 4.336 |
+------------------+--------------+------------+-----------+-------+


Quantizing layer 32/32..
+------------------+--------------+------------+-----------+-------+
|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |
+==================+==============+============+===========+=======+
| self_attn.k_proj | 98552.875    | -          | -         | 1.565 |
| self_attn.v_proj | 69270.344    | -          | -         | 1.227 |
| self_attn.q_proj | 93488.078    | -          | -         | 1.283 |
| self_attn.o_proj | 11892.392    | -          | -         | 1.500 |
| mlp.up_proj      | 168187.719   | -          | -         | 1.595 |
| mlp.gate_proj    | 188633.672   | -          | -         | 1.300 |
| mlp.down_proj    | 152897.375   | -          | -         | 4.434 |
+------------------+--------------+------------+-----------+-------+


753.1702768802643
save quantized tensors.
wikitext2
PPL:  6.283639907836914
ptb
PPL:  27.833810806274414
c4
PPL:  7.830746173858643
