CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 0 --tensor-name self_attn.q_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_0.self_attn.q_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 0 --tensor-name self_attn.k_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_0.self_attn.k_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 0 --tensor-name self_attn.v_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_0.self_attn.v_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 0 --tensor-name self_attn.o_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_0.self_attn.o_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 0 --tensor-name mlp.up_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_0.mlp.up_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 0 --tensor-name mlp.down_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_0.mlp.down_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 0 --tensor-name mlp.gate_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_0.mlp.gate_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 1 --tensor-name self_attn.q_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_1.self_attn.q_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 1 --tensor-name self_attn.k_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_1.self_attn.k_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 1 --tensor-name self_attn.v_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_1.self_attn.v_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 1 --tensor-name self_attn.o_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_1.self_attn.o_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 1 --tensor-name mlp.up_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_1.mlp.up_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 1 --tensor-name mlp.down_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_1.mlp.down_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 1 --tensor-name mlp.gate_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_1.mlp.gate_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 2 --tensor-name self_attn.q_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_2.self_attn.q_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 2 --tensor-name self_attn.k_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_2.self_attn.k_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 2 --tensor-name self_attn.v_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_2.self_attn.v_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 2 --tensor-name self_attn.o_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_2.self_attn.o_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 2 --tensor-name mlp.up_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_2.mlp.up_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 2 --tensor-name mlp.down_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_2.mlp.down_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 2 --tensor-name mlp.gate_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_2.mlp.gate_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 3 --tensor-name self_attn.q_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_3.self_attn.q_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 3 --tensor-name self_attn.k_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_3.self_attn.k_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 3 --tensor-name self_attn.v_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_3.self_attn.v_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 3 --tensor-name self_attn.o_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_3.self_attn.o_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 3 --tensor-name mlp.up_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_3.mlp.up_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 3 --tensor-name mlp.down_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_3.mlp.down_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 3 --tensor-name mlp.gate_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_3.mlp.gate_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 4 --tensor-name self_attn.q_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_4.self_attn.q_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 4 --tensor-name self_attn.k_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_4.self_attn.k_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 4 --tensor-name self_attn.v_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_4.self_attn.v_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 4 --tensor-name self_attn.o_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_4.self_attn.o_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 4 --tensor-name mlp.up_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_4.mlp.up_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 4 --tensor-name mlp.down_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_4.mlp.down_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 4 --tensor-name mlp.gate_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_4.mlp.gate_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 5 --tensor-name self_attn.q_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_5.self_attn.q_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 5 --tensor-name self_attn.k_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_5.self_attn.k_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 5 --tensor-name self_attn.v_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_5.self_attn.v_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 5 --tensor-name self_attn.o_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_5.self_attn.o_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 5 --tensor-name mlp.up_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_5.mlp.up_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 5 --tensor-name mlp.down_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_5.mlp.down_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 5 --tensor-name mlp.gate_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_5.mlp.gate_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 6 --tensor-name self_attn.q_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_6.self_attn.q_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 6 --tensor-name self_attn.k_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_6.self_attn.k_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 6 --tensor-name self_attn.v_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_6.self_attn.v_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 6 --tensor-name self_attn.o_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_6.self_attn.o_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 6 --tensor-name mlp.up_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_6.mlp.up_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 6 --tensor-name mlp.down_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_6.mlp.down_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 6 --tensor-name mlp.gate_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_6.mlp.gate_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 7 --tensor-name self_attn.q_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_7.self_attn.q_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 7 --tensor-name self_attn.k_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_7.self_attn.k_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 7 --tensor-name self_attn.v_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_7.self_attn.v_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 7 --tensor-name self_attn.o_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_7.self_attn.o_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 7 --tensor-name mlp.up_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_7.mlp.up_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 7 --tensor-name mlp.down_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_7.mlp.down_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 7 --tensor-name mlp.gate_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_7.mlp.gate_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 8 --tensor-name self_attn.q_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_8.self_attn.q_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 8 --tensor-name self_attn.k_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_8.self_attn.k_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 8 --tensor-name self_attn.v_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_8.self_attn.v_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 8 --tensor-name self_attn.o_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_8.self_attn.o_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 8 --tensor-name mlp.up_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_8.mlp.up_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 8 --tensor-name mlp.down_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_8.mlp.down_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 8 --tensor-name mlp.gate_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_8.mlp.gate_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 9 --tensor-name self_attn.q_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_9.self_attn.q_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 9 --tensor-name self_attn.k_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_9.self_attn.k_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 9 --tensor-name self_attn.v_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_9.self_attn.v_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 9 --tensor-name self_attn.o_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_9.self_attn.o_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 9 --tensor-name mlp.up_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_9.mlp.up_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 9 --tensor-name mlp.down_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_9.mlp.down_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 9 --tensor-name mlp.gate_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_9.mlp.gate_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 10 --tensor-name self_attn.q_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_10.self_attn.q_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 10 --tensor-name self_attn.k_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_10.self_attn.k_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 10 --tensor-name self_attn.v_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_10.self_attn.v_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 10 --tensor-name self_attn.o_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_10.self_attn.o_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 10 --tensor-name mlp.up_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_10.mlp.up_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 10 --tensor-name mlp.down_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_10.mlp.down_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 10 --tensor-name mlp.gate_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_10.mlp.gate_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 11 --tensor-name self_attn.q_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_11.self_attn.q_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 11 --tensor-name self_attn.k_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_11.self_attn.k_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 11 --tensor-name self_attn.v_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_11.self_attn.v_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 11 --tensor-name self_attn.o_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_11.self_attn.o_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 11 --tensor-name mlp.up_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_11.mlp.up_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 11 --tensor-name mlp.down_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_11.mlp.down_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 11 --tensor-name mlp.gate_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_11.mlp.gate_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 12 --tensor-name self_attn.q_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_12.self_attn.q_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 12 --tensor-name self_attn.k_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_12.self_attn.k_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 12 --tensor-name self_attn.v_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_12.self_attn.v_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 12 --tensor-name self_attn.o_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_12.self_attn.o_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 12 --tensor-name mlp.up_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_12.mlp.up_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 12 --tensor-name mlp.down_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_12.mlp.down_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 12 --tensor-name mlp.gate_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_12.mlp.gate_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 13 --tensor-name self_attn.q_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_13.self_attn.q_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 13 --tensor-name self_attn.k_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_13.self_attn.k_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 13 --tensor-name self_attn.v_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_13.self_attn.v_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 13 --tensor-name self_attn.o_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_13.self_attn.o_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 13 --tensor-name mlp.up_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_13.mlp.up_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 13 --tensor-name mlp.down_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_13.mlp.down_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 13 --tensor-name mlp.gate_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_13.mlp.gate_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 14 --tensor-name self_attn.q_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_14.self_attn.q_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 14 --tensor-name self_attn.k_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_14.self_attn.k_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 14 --tensor-name self_attn.v_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_14.self_attn.v_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 14 --tensor-name self_attn.o_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_14.self_attn.o_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 14 --tensor-name mlp.up_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_14.mlp.up_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 14 --tensor-name mlp.down_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_14.mlp.down_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 14 --tensor-name mlp.gate_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_14.mlp.gate_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 15 --tensor-name self_attn.q_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_15.self_attn.q_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 15 --tensor-name self_attn.k_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_15.self_attn.k_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 15 --tensor-name self_attn.v_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_15.self_attn.v_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 15 --tensor-name self_attn.o_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_15.self_attn.o_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 15 --tensor-name mlp.up_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_15.mlp.up_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 15 --tensor-name mlp.down_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_15.mlp.down_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 15 --tensor-name mlp.gate_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_15.mlp.gate_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 16 --tensor-name self_attn.q_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_16.self_attn.q_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 16 --tensor-name self_attn.k_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_16.self_attn.k_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 16 --tensor-name self_attn.v_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_16.self_attn.v_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 16 --tensor-name self_attn.o_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_16.self_attn.o_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 16 --tensor-name mlp.up_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_16.mlp.up_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 16 --tensor-name mlp.down_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_16.mlp.down_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 16 --tensor-name mlp.gate_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_16.mlp.gate_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 17 --tensor-name self_attn.q_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_17.self_attn.q_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 17 --tensor-name self_attn.k_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_17.self_attn.k_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 17 --tensor-name self_attn.v_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_17.self_attn.v_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 17 --tensor-name self_attn.o_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_17.self_attn.o_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 17 --tensor-name mlp.up_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_17.mlp.up_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 17 --tensor-name mlp.down_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_17.mlp.down_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 17 --tensor-name mlp.gate_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_17.mlp.gate_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 18 --tensor-name self_attn.q_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_18.self_attn.q_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 18 --tensor-name self_attn.k_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_18.self_attn.k_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 18 --tensor-name self_attn.v_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_18.self_attn.v_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 18 --tensor-name self_attn.o_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_18.self_attn.o_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 18 --tensor-name mlp.up_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_18.mlp.up_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 18 --tensor-name mlp.down_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_18.mlp.down_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 18 --tensor-name mlp.gate_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_18.mlp.gate_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 19 --tensor-name self_attn.q_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_19.self_attn.q_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 19 --tensor-name self_attn.k_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_19.self_attn.k_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 19 --tensor-name self_attn.v_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_19.self_attn.v_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 19 --tensor-name self_attn.o_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_19.self_attn.o_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 19 --tensor-name mlp.up_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_19.mlp.up_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 19 --tensor-name mlp.down_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_19.mlp.down_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 19 --tensor-name mlp.gate_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_19.mlp.gate_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 20 --tensor-name self_attn.q_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_20.self_attn.q_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 20 --tensor-name self_attn.k_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_20.self_attn.k_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 20 --tensor-name self_attn.v_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_20.self_attn.v_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 20 --tensor-name self_attn.o_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_20.self_attn.o_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 20 --tensor-name mlp.up_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_20.mlp.up_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 20 --tensor-name mlp.down_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_20.mlp.down_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 20 --tensor-name mlp.gate_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_20.mlp.gate_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 21 --tensor-name self_attn.q_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_21.self_attn.q_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 21 --tensor-name self_attn.k_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_21.self_attn.k_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 21 --tensor-name self_attn.v_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_21.self_attn.v_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 21 --tensor-name self_attn.o_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_21.self_attn.o_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 21 --tensor-name mlp.up_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_21.mlp.up_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 21 --tensor-name mlp.down_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_21.mlp.down_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 21 --tensor-name mlp.gate_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_21.mlp.gate_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 22 --tensor-name self_attn.q_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_22.self_attn.q_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 22 --tensor-name self_attn.k_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_22.self_attn.k_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 22 --tensor-name self_attn.v_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_22.self_attn.v_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 22 --tensor-name self_attn.o_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_22.self_attn.o_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 22 --tensor-name mlp.up_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_22.mlp.up_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 22 --tensor-name mlp.down_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_22.mlp.down_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 22 --tensor-name mlp.gate_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_22.mlp.gate_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 23 --tensor-name self_attn.q_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_23.self_attn.q_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 23 --tensor-name self_attn.k_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_23.self_attn.k_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 23 --tensor-name self_attn.v_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_23.self_attn.v_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 23 --tensor-name self_attn.o_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_23.self_attn.o_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 23 --tensor-name mlp.up_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_23.mlp.up_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 23 --tensor-name mlp.down_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_23.mlp.down_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 23 --tensor-name mlp.gate_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_23.mlp.gate_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 24 --tensor-name self_attn.q_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_24.self_attn.q_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 24 --tensor-name self_attn.k_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_24.self_attn.k_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 24 --tensor-name self_attn.v_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_24.self_attn.v_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 24 --tensor-name self_attn.o_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_24.self_attn.o_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 24 --tensor-name mlp.up_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_24.mlp.up_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 24 --tensor-name mlp.down_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_24.mlp.down_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 24 --tensor-name mlp.gate_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_24.mlp.gate_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 25 --tensor-name self_attn.q_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_25.self_attn.q_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 25 --tensor-name self_attn.k_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_25.self_attn.k_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 25 --tensor-name self_attn.v_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_25.self_attn.v_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 25 --tensor-name self_attn.o_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_25.self_attn.o_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 25 --tensor-name mlp.up_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_25.mlp.up_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 25 --tensor-name mlp.down_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_25.mlp.down_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 25 --tensor-name mlp.gate_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_25.mlp.gate_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 26 --tensor-name self_attn.q_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_26.self_attn.q_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 26 --tensor-name self_attn.k_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_26.self_attn.k_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 26 --tensor-name self_attn.v_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_26.self_attn.v_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 26 --tensor-name self_attn.o_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_26.self_attn.o_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 26 --tensor-name mlp.up_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_26.mlp.up_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 26 --tensor-name mlp.down_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_26.mlp.down_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 26 --tensor-name mlp.gate_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_26.mlp.gate_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 27 --tensor-name self_attn.q_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_27.self_attn.q_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 27 --tensor-name self_attn.k_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_27.self_attn.k_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 27 --tensor-name self_attn.v_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_27.self_attn.v_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 27 --tensor-name self_attn.o_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_27.self_attn.o_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 27 --tensor-name mlp.up_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_27.mlp.up_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 27 --tensor-name mlp.down_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_27.mlp.down_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 27 --tensor-name mlp.gate_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_27.mlp.gate_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 28 --tensor-name self_attn.q_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_28.self_attn.q_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 28 --tensor-name self_attn.k_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_28.self_attn.k_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 28 --tensor-name self_attn.v_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_28.self_attn.v_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 28 --tensor-name self_attn.o_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_28.self_attn.o_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 28 --tensor-name mlp.up_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_28.mlp.up_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 28 --tensor-name mlp.down_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_28.mlp.down_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 28 --tensor-name mlp.gate_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_28.mlp.gate_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 29 --tensor-name self_attn.q_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_29.self_attn.q_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 29 --tensor-name self_attn.k_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_29.self_attn.k_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 29 --tensor-name self_attn.v_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_29.self_attn.v_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 29 --tensor-name self_attn.o_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_29.self_attn.o_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 29 --tensor-name mlp.up_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_29.mlp.up_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 29 --tensor-name mlp.down_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_29.mlp.down_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 29 --tensor-name mlp.gate_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_29.mlp.gate_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 30 --tensor-name self_attn.q_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_30.self_attn.q_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 30 --tensor-name self_attn.k_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_30.self_attn.k_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 30 --tensor-name self_attn.v_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_30.self_attn.v_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 30 --tensor-name self_attn.o_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_30.self_attn.o_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 30 --tensor-name mlp.up_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_30.mlp.up_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 30 --tensor-name mlp.down_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_30.mlp.down_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 30 --tensor-name mlp.gate_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_30.mlp.gate_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 31 --tensor-name self_attn.q_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_31.self_attn.q_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 31 --tensor-name self_attn.k_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_31.self_attn.k_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 31 --tensor-name self_attn.v_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_31.self_attn.v_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 31 --tensor-name self_attn.o_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_31.self_attn.o_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 31 --tensor-name mlp.up_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_31.mlp.up_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 31 --tensor-name mlp.down_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_31.mlp.down_proj.txt
CUDA_VISIBLE_DEVICES=0 python llama.py /home/v-wentaoni/workspace/llama-recipes/llama-2-7b-hf c4 --wbits 16 --true-sequential --act-order --groupsize 128 --single-tensor-quant-benchmark --layer-idx 31 --tensor-name mlp.gate_proj > /home/v-wentaoni/workspace/amlt/gptq_per_tensor_16bit/layer_31.mlp.gate_proj.txt
